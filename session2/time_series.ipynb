{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVhPZfkbyPoW"
      },
      "source": [
        "# Time-series Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51ihHWF2ns01"
      },
      "source": [
        "A time series is a sequence of observations, ordered in time. Forecasting involves training a model on historical data and using them to predict future observations. A simple example is a linear auto-regressive model. The linear auto-regressive (AR) model of a time-series $Z_t$ with $t=1,2,\\dots,\\infty$ is given by\n",
        "\n",
        "$$\\hat{z}_t = a_1 z_{t-1} + a_2 z_{t-2} + \\cdots + a_p z_{t-p},$$\n",
        "\n",
        "with $a_i \\in \\mathbb{R}$ for $i=1, \\dots, p$ and $p$ the model lag. The prediction for a certain time $t$ is equal to a weighted sum of the previous values up to a certain lag $p$. In a similar way, the nonlinear (NAR) variant is described as\n",
        "\n",
        "$$\\hat{z}_t = f(z_{t-1}, z_{t-2}, \\dots, z_{t-p}).$$\n",
        "\n",
        "The figure below visualizes this process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlur6AxPpiuF"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/KULasagna/ANN_DL_public/master/assets/nar.jpg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drsFGMT3nvb9"
      },
      "source": [
        "Remark that in this way, the time-series identification can be written as a classical black-box regression modeling problem $\\hat{y}_t=f(x_t)$ with $y_t=z_t$ and $x_t=[z_{t-1}, z_{t-2}, \\dots, z_{t-p}]$. When preparing the dataset and applying train/validation/test splits, it is important to prevent *data leakage* by respecting the temporal information flow. More precisely, a datapoint $z_t$ should not be part of two splits &mdash; either as input $x_t$ or target $y_t$ &mdash; and training (or validation) sets should not contain datapoints that occur after test datapoints.\n",
        "\n",
        "In this notebook, we work on the time-series prediction problem using a multilayer perceptron (MLP) and a long short-term memory network (LSTM)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq7pzE-qs2cm"
      },
      "source": [
        "## Colab Setup\n",
        "This part is only required when running this notebook \"in the cloud\" on [Google Colab](https://colab.research.google.com). When running it locally, skip this part and go to the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jLmJE3TEs8-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12dd2cc2-f9a0-4da8-8eae-4a9f9e5724a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-26 10:23:04--  https://raw.githubusercontent.com/KULasagna/ANN_DL_public/master/session2/SantaFe.npz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1598 (1.6K) [application/octet-stream]\n",
            "Saving to: ‘SantaFe.npz.2’\n",
            "\n",
            "\rSantaFe.npz.2         0%[                    ]       0  --.-KB/s               \rSantaFe.npz.2       100%[===================>]   1.56K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-07-26 10:23:04 (20.5 MB/s) - ‘SantaFe.npz.2’ saved [1598/1598]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load some auxiliary files from github.\n",
        "!wget https://raw.githubusercontent.com/KULasagna/ANN_DL_public/master/session2/SantaFe.npz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5kT1d__qweW"
      },
      "source": [
        "## Setup\n",
        "Import all the necessary modules used throughout this notebook and define some helper methods to work with timeseries data and visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zFNzy5EJqy0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85ca0041-020b-4613-ce08-336b265d16df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.12.1 in /usr/local/lib/python3.11/dist-packages (2.12.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (1.73.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (0.4.38)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (18.1.1)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.1) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.1) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.38,>=0.4.38 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.1) (0.4.38)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.1) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.1) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.1) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.1) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.1) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.1) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.1) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.1) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.1) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.1) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.1) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.1) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.1) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.1) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.1) (3.3.1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'compute_average_loss' from 'tensorflow.python.ops.nn_impl' (/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/nn_impl.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-1689086330.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install tensorflow==2.12.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmlir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch_norm_with_global_normalization_v2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbatch_norm_with_global_normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch_normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_average_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdepthwise_conv2d_v2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdepthwise_conv2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0ml2_normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'compute_average_loss' from 'tensorflow.python.ops.nn_impl' (/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/nn_impl.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Import the required modules for this notebook\n",
        "%pip install tensorflow==2.12.1\n",
        "from dataclasses import dataclass\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "from sklearn.model_selection import TimeSeriesSplit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCPfrPvhhwLQ"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Fold:\n",
        "  train_idxs: np.ndarray\n",
        "  val_idxs: np.ndarray\n",
        "\n",
        "def prepare_timeseries(timeseries, lag, validation_size=0, validation_folds=0):\n",
        "  # Generate train (and validation) sets for the given timeseries and lag\n",
        "  data = scipy.linalg.hankel(timeseries[:-lag], timeseries[-lag-1:-1])\n",
        "  targets = timeseries[lag:]\n",
        "  if validation_size > 0 and validation_folds > 0:\n",
        "    tss = TimeSeriesSplit(test_size=validation_size, gap=lag)\n",
        "    tss.n_splits = validation_folds\n",
        "    folds = [Fold(train_idxs, val_idxs) for (train_idxs, val_idxs) in tss.split(data)]\n",
        "    return data, targets, folds\n",
        "  return data, targets\n",
        "\n",
        "def shift(window, values):\n",
        "  # Append new values to the given window (dropping the oldest values)\n",
        "  result = np.empty(window.shape)\n",
        "  values = np.atleast_1d(values)\n",
        "  s = values.shape[0]\n",
        "  result[:-s] = window[s:]\n",
        "  result[-s:] = values\n",
        "  return result\n",
        "\n",
        "def normalize(timeseries, params=None):\n",
        "  # Apply z-score normalization to the given timeseries\n",
        "  if params is None:\n",
        "    params = (np.mean(timeseries), np.std(timeseries))\n",
        "  mu, sigma = params\n",
        "  normalized = (timeseries - mu) / sigma\n",
        "  return normalized, params\n",
        "\n",
        "def rescale(timeseries, params):\n",
        "  # Rescale the normalized timeseries back to its original values\n",
        "  mu, sigma = params\n",
        "  rescaled = mu + timeseries * sigma\n",
        "  return rescaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNiSpu-WiwIV"
      },
      "outputs": [],
      "source": [
        "def plot_history(history, title, filename=None):\n",
        "  # Plot the train and validation loss curves\n",
        "  fig, ax = plt.subplots(figsize=(10, 3))\n",
        "  ax.semilogy(history.history['loss'], label='Train')\n",
        "  if 'val_loss' in history.history:\n",
        "    ax.semilogy(history.history['val_loss'], label='Validation')\n",
        "  ax.legend()\n",
        "  ax.set_xlabel('Epochs')\n",
        "  ax.set_ylabel('Loss')\n",
        "  plt.title(title)\n",
        "  if filename is not None:\n",
        "    plt.savefig(f\"{filename}.svg\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_timeseries(timeseries_dict, title, filename=None):\n",
        "  # Plot the given timeseries\n",
        "  fig, ax = plt.subplots(figsize=(10, 3))\n",
        "  for label, (start, ts) in timeseries_dict.items():\n",
        "    ax.plot(start + np.arange(len(ts)), ts, label=label)\n",
        "  ax.legend()\n",
        "  ax.set_xlabel('Timestep')\n",
        "  ax.set_ylabel('Laser intensity')\n",
        "  plt.title(title)\n",
        "  if filename is not None:\n",
        "    plt.savefig(f\"{filename}.svg\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qGqFrCZpbfd"
      },
      "source": [
        "## Santa Fe Laser Dataset\n",
        "The Santa Fe laser dataset is obtained from a chaotic laser which can be described as a nonlinear dynamical system. The first $1000$ data points can be used for training and validation purposes. The aim is to predict the next $100$ points (it is forbidden to include these points in the training or validation sets!). Both datasets are stored in the `SantaFe.npz` file and are visualized below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pTA6Tb2tVCv"
      },
      "outputs": [],
      "source": [
        "santafe = np.load('SantaFe.npz')\n",
        "train_series = santafe['A']\n",
        "test_series = santafe['Acont']\n",
        "plot_timeseries({\n",
        "    'Train': (0, train_series),\n",
        "    'Test': (len(train_series), test_series)\n",
        "}, 'Santa Fe laser dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEPf2pX1ua62"
      },
      "source": [
        "To train the various nonlinear autoregressive models, it will be useful to prepare the timeseries dataset beforehand. We start by normalizing the train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "551MMyXmRXBf"
      },
      "outputs": [],
      "source": [
        "normalized, params = normalize(santafe['A'])\n",
        "normalized_test, _ = normalize(santafe['Acont'], params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32tsE_fWcJHH"
      },
      "source": [
        "Next, the `prepare_timeseries` function is used to convert the timeseries into training data ($x_t$) and targets ($y_t$). Make sure you understand what the function does by trying it out on a small toy example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziSIfh35sxYC"
      },
      "outputs": [],
      "source": [
        "timeseries = np.arange(10)  # Increase this number to create a larger time series\n",
        "lag = 3  # Try different values for the lag\n",
        "data, targets, folds = prepare_timeseries(timeseries, lag, validation_size=1, validation_folds=2)  # Examine the effect of the validation size and number of validation folds\n",
        "print(f\"Original timeseries: {timeseries}\")\n",
        "print(f\"Data for lag {lag}: {data}\")\n",
        "print(f\"Targets: {targets}\")\n",
        "for i, fold in enumerate(folds):\n",
        "  print(f\"Fold {i}:\")\n",
        "  print(f\"  Train:     data={data[fold.train_idxs]}, targets={targets[fold.train_idxs]}\")\n",
        "  print(f\"  Validate:  data={data[fold.val_idxs]}, targets={targets[fold.val_idxs]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzSE4nW_dVGQ"
      },
      "source": [
        "Once you understand the dataset structure, apply it to the normalized Santa Fe timeseries. You can come back to these cells later to change the values of the lag or validation parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzkVFMz2dmdz"
      },
      "outputs": [],
      "source": [
        "# @title Parameters { run: \"auto\" }\n",
        "lag = 30 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "validation_size = 100 # @param {type:\"slider\", min:10, max:200, step:1}\n",
        "validation_folds = 6 # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "# This is the hidden dimension of the MLP and LSTM networks trained later in this notebook\n",
        "H = 70 # @param {type:\"slider\", min:10, max:100, step:1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiRjApLy4HYg"
      },
      "outputs": [],
      "source": [
        "data, targets, folds = prepare_timeseries(normalized, lag, validation_size, validation_folds)\n",
        "# Plot train and validation data for each fold:\n",
        "for f, fold in enumerate(folds):\n",
        "  train_series = np.concatenate([data[fold.train_idxs[0]], targets[fold.train_idxs]])\n",
        "  val_series = np.concatenate([data[fold.val_idxs[0]], targets[fold.val_idxs]])\n",
        "  plot_timeseries({\"Train\": (0, train_series), \"Validation\": (len(train_series), val_series)}, f\"Fold {f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYqrMMAjtQFv"
      },
      "source": [
        "## MLP\n",
        "We now train a first nonlinear autoregressive (NAR) model on the training set and choose the hyperparameters based on the predictive performance on the validation sets. This first model is a multilayer perceptron (MLP) and training is done in feedforward mode using the prepared training set\n",
        "\n",
        "$$\\hat{z}_t = w^\\top \\tanh(V[z_{t-1}; z_{t-2}; \\dots; z_{t-p}] + \\beta).$$\n",
        "\n",
        "In order to make predictions, the trained network is used in an iterative way as a recurrent network\n",
        "\n",
        "$$\\hat{z}_t = w^\\top \\tanh(V[\\hat{z}_{t-1}; \\hat{z}_{t-2}; \\dots; \\hat{z}_{t-p}] + \\beta).$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fx-b4pKJ17iy"
      },
      "outputs": [],
      "source": [
        "# Define the MLP network architecture\n",
        "def MLP(input_dim, hidden_dim, output_dim, activation='tanh'):\n",
        "  return keras.Sequential([\n",
        "    keras.layers.Input(shape=[input_dim]),  # Expect input of shape (B, I) with B batch size, I input size\n",
        "    keras.layers.Dense(units=hidden_dim, activation=activation),  # Output of shape (B, H) with H hidden feature size\n",
        "    keras.layers.Dense(units=output_dim)  # Output of shape (B, O) with O output size\n",
        "  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ftoGLnzbNX"
      },
      "source": [
        "Train the MLP on the last training fold. You can later put this code in a `for` loop to train on each of the training folds for determining the optimal hyperparameters using $N$-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u55ZltdH-LgP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "net = MLP(lag, H, 1)\n",
        "net.compile(\n",
        "  loss=keras.losses.MeanSquaredError(),\n",
        "  optimizer=keras.optimizers.Adam(learning_rate=0.01)\n",
        ")\n",
        "for i in range(validation_folds):\n",
        "  fold = folds[i]\n",
        "  history = net.fit(data[fold.train_idxs], targets[fold.train_idxs],\n",
        "                  validation_data=(data[fold.val_idxs], targets[fold.val_idxs]),\n",
        "                  epochs=200, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdqT_7mpz82I"
      },
      "source": [
        "Plot the train and validation loss curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFTkxh0dzRoL"
      },
      "outputs": [],
      "source": [
        "plot_history(history, \"MLP training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtbbqdB73CLO"
      },
      "source": [
        "### **Exercise 1**\n",
        "Investigate the model performance with different lags and number of neurons. Discuss how the model looks and explain clearly how you tune the parameters and what the influence on the final prediction is. Which combination of parameters gives the best performance (MSE) on the test set?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the hyperparameter grid for lag and H\n",
        "lag_values = [10, 20, 30, 40, 50]\n",
        "H_values = [10, 20, 30, 50, 70, 90, 100]\n",
        "validation_size = 70\n",
        "validation_folds = 10\n",
        "\n",
        "# Initialize a matrix to store the average MSE values across all folds\n",
        "average_mse_values = np.zeros((len(lag_values), len(H_values)))\n",
        "\n",
        "# Iterate over the hyperparameter grid for lag and H\n",
        "for i, lag in enumerate(lag_values):\n",
        "    for j, H in enumerate(H_values):\n",
        "        print(f\"Training model with lag={lag} and H={H}\")\n",
        "\n",
        "        # Prepare the time series data with the current lag value\n",
        "        data, targets, folds = prepare_timeseries(normalized, lag, validation_size, validation_folds)\n",
        "\n",
        "        # Initialize a list to store the MSE values for each fold\n",
        "        fold_mse_values = []\n",
        "\n",
        "        # Iterate over each fold\n",
        "        for fold_idx, fold in enumerate(folds):\n",
        "            print(f\"  Fold {fold_idx + 1}/{len(folds)}\")\n",
        "\n",
        "            # Initialize and train the model with the current hyperparameters\n",
        "            net = MLP(lag, H, 1)  # Ensure input_dim matches the lag value\n",
        "            net.compile(loss=keras.losses.MeanSquaredError(), optimizer=keras.optimizers.Adam(learning_rate=0.01))\n",
        "\n",
        "            # Train the model and calculate the MSE on the validation set\n",
        "            history = net.fit(\n",
        "                data[fold.train_idxs], targets[fold.train_idxs],\n",
        "                validation_data=(data[fold.val_idxs], targets[fold.val_idxs]),\n",
        "                epochs=200, verbose=0\n",
        "            )\n",
        "\n",
        "            # Store the MSE value for the current fold\n",
        "            fold_mse_values.append(history.history['val_loss'][-1])  # Using the last validation loss as MSE\n",
        "\n",
        "        # Calculate the average MSE across all folds\n",
        "        average_mse_values[i, j] = np.mean(fold_mse_values)\n",
        "        print(f\"Average MSE for lag={lag} and H={H}: {average_mse_values[i, j]}\")\n",
        "\n",
        "# Create a line plot to visualize the average MSE values for lag and H\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, lag in enumerate(lag_values):\n",
        "    plt.plot(H_values, average_mse_values[i, :], marker='o', label=f'Lag={lag}')\n",
        "\n",
        "plt.xlabel('Hidden Dimension (H)')\n",
        "plt.ylabel('Average MSE')\n",
        "plt.title('Average MSE for Different Lag and H Settings Across All Folds')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "dtynrPCTX0bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best combination of lag and H based on the minimum average MSE\n",
        "best_mse = np.min(average_mse_values)\n",
        "best_lag_index, best_H_index = np.unravel_index(np.argmin(average_mse_values), average_mse_values.shape)\n",
        "best_lag = lag_values[best_lag_index]\n",
        "best_H = H_values[best_H_index]\n",
        "\n",
        "print(f\"\\nBest combination of lag and H based on average MSE: lag={best_lag}, H={best_H} with MSE={best_mse:.3f}\")"
      ],
      "metadata": {
        "id": "plIdeVL8PBj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vf8lWCXCTlK"
      },
      "outputs": [],
      "source": [
        "# Evaluation on test set\n",
        "# Use the best lag and H values found during hyperparameter tuning\n",
        "lag = 30\n",
        "H = 70\n",
        "\n",
        "# Prepare data for the best lag\n",
        "data, targets = prepare_timeseries(normalized, lag)\n",
        "\n",
        "# Retrain the MLP model with the best lag and H values\n",
        "net = MLP(lag, H, 1)\n",
        "net.compile(\n",
        "  loss=keras.losses.MeanSquaredError(),\n",
        "  optimizer=keras.optimizers.Adam(learning_rate=0.01)\n",
        ")\n",
        "\n",
        "# Train the model on the entire training data with the best lag\n",
        "net.fit(data, targets, epochs=200, verbose=0)\n",
        "\n",
        "\n",
        "test_data = shift(data[-1], targets[-1])\n",
        "predictions = np.empty(normalized_test.shape)\n",
        "for t in range(len(predictions)):\n",
        "  predictions[t] = net.predict(test_data.reshape((1, lag)), verbose=0).squeeze()\n",
        "  test_data = shift(test_data, predictions[t])\n",
        "\n",
        "# Rescale predictions\n",
        "predictions_mlp = rescale(predictions, params)\n",
        "\n",
        "# Compute the mean squared error between the predictions and test set\n",
        "mse = np.mean((test_series - predictions_mlp)**2)\n",
        "print(\"The MSE on the test set is: {:.3f}\".format(mse))\n",
        "\n",
        "# Plot\n",
        "plot_timeseries({\"Test\": (1000, test_series), \"Predictions\": (1000, predictions_mlp)}, \"MLP prediction results on continuation of Santa Fe laser dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLskk7m_osSv"
      },
      "source": [
        "## LSTM\n",
        "We now train the second model, which is a Long Short Term Memory (LSTM) network. These are a special kind of RNN, capable of learning long-term dependencies. LSTMs contain information outside the normal flow of the recurrent network in a gated cell. Information can be stored in, written to, or read from a cell, much like data in a computer's memory. The cell makes decisions about what to store and when to allow reads, writes and erasures, via gates that open and close. Those gates act on the signals they receive, and similar to the neural network's nodes, they block or pass on information based on its strength and importance, which they filter with their own sets of weights. Those weights, like the weights that modulate input and hidden states, are adjusted via the recurrent network's learning process. That is, the cells learn when to allow data to enter, leave or be deleted through the iterative process of making guesses, backpropagating the error, and adjusting weights via gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2DT-JT3ypAG"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM network structure\n",
        "def LSTM(input_dim, hidden_dim, output_dim):\n",
        "  return keras.Sequential([\n",
        "    keras.layers.Input(shape=[None, input_dim], batch_size=1),  # LSTM layer expects input of shape (B, T, F) with B batch size, T timesteps, F feature size\n",
        "    keras.layers.LSTM(units=hidden_dim, return_sequences=True, stateful=True),  # Output of shape (B, T, H) with H hidden state size\n",
        "    keras.layers.Dense(units=output_dim)  # Output of shape (B, T, O) with O output size\n",
        "  ])\n",
        "\n",
        "class LSTMCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    self.model.reset_states()  # Make sure the LSTM's hidden state is reset after every epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRK4-A3n7y-r"
      },
      "source": [
        "Train the LSTM on the last training fold. You can later put this code in a `for` loop to train on each of the training folds for determining the optimal hyperparameters using $N$-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d6aj03us0xK"
      },
      "outputs": [],
      "source": [
        "net = LSTM(lag, H, 1)\n",
        "net.compile(\n",
        "  loss=keras.losses.MeanSquaredError(),\n",
        "  optimizer=keras.optimizers.Adam(learning_rate=0.001)\n",
        ")\n",
        "fold = folds[-1]\n",
        "history = net.fit(\n",
        "    data[fold.train_idxs].reshape((1, -1, lag)), targets[fold.train_idxs].reshape((1, -1)),\n",
        "    validation_data=(data[fold.val_idxs].reshape((1, -1, lag)), targets[fold.val_idxs].reshape((1, -1))),\n",
        "    epochs=150, callbacks=[LSTMCallback()], verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Oz7_At__Ax3"
      },
      "source": [
        "Plot the train and validation loss curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlzuuU4MXqTs"
      },
      "outputs": [],
      "source": [
        "plot_history(history, \"LSTM training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpZ1p-B4_vPz"
      },
      "source": [
        "### **Exercise 2**\n",
        "Investigate the model performance with different lags and number of hidden states. Discuss how the model looks and explain clearly how you tune the parameters and what the influence on the final prediction is. Which combination of parameters gives the best performance (MSE) on the test set?\n",
        "\n",
        "Compare the results of the recurrent MLP with the LSTM. Which model do you prefer and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dduj4Rbi6tza"
      },
      "outputs": [],
      "source": [
        "# LSTM Evaluation\n",
        "# Use the best lag and H values found during hyperparameter tuning\n",
        "lag = best_lag\n",
        "H = best_H\n",
        "\n",
        "# Prepare data for the best lag\n",
        "data, targets = prepare_timeseries(normalized, lag)\n",
        "\n",
        "net.reset_states()\n",
        "# Need to reshape data for LSTM prediction\n",
        "net.predict(data.reshape(1, -1, lag), verbose=0)\n",
        "test_data = shift(data[-1], targets[-1])\n",
        "predictions = np.empty(normalized_test.shape)\n",
        "for t in range(len(predictions)):\n",
        "  # Need to reshape test_data for LSTM prediction\n",
        "  predictions[t] = net.predict(test_data.reshape((1, 1, lag)), verbose=0).squeeze()\n",
        "  test_data = shift(test_data, predictions[t])\n",
        "\n",
        "# Rescale predictions\n",
        "predictions_lstm = rescale(predictions, params)\n",
        "\n",
        "# Compute the mean squared error between the predictions and test set\n",
        "mse = np.mean((test_series - predictions_lstm)**2)\n",
        "print(\"The MSE on the test set is: {:.3f}\".format(mse))\n",
        "\n",
        "# Plot\n",
        "plot_timeseries({\"Test\": (1000, test_series), \"Predictions\": (1000, predictions_lstm)}, \"LSTM prediction results on continuation of Santa Fe laser dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf-BQY0UBK6A"
      },
      "outputs": [],
      "source": [
        "# Comparison of both models\n",
        "plot_timeseries({\"Test\": (1000, test_series), \"MLP\": (1000, predictions_mlp), \"LSTM\": (1000, predictions_lstm)}, \"Prediction results on continuation of Santa Fe laser dataset\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}